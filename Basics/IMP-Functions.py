Useful PySpark Functions (like size())



Whenever you use functions like col, lit, when, split, size, etc. â€” make sure to import them first:
from pyspark.sql.functions import col, lit, when, split, size, upper, length, round


1. size() â†’ ðŸ“ Count elements in an array
 
from pyspark.sql.functions import size, split

df.withColumn("marks_count", size(split(col("marks"), ",")))



2. length() â†’ ðŸ”¡ Length of a string
 
from pyspark.sql.functions import length

df.withColumn("name_length", length(col("name")))



3. split() â†’ âœ‚ï¸ Split a string into an array
 
from pyspark.sql.functions import split

df.withColumn("marks_array", split(col("marks"), ","))



4. concat() â†’ ðŸ”— Concatenate multiple columns
 
from pyspark.sql.functions import concat, lit

df.withColumn("full_name", concat(col("first_name"), lit(" "), col("last_name")))



5. substring() â†’ ðŸ§µ Extract part of a string
 
from pyspark.sql.functions import substring

df.withColumn("first_3_letters", substring(col("name"), 1, 3))



6. upper() / lower() â†’ ðŸ”  Case conversion
 
from pyspark.sql.functions import upper, lower

df.withColumn("name_upper", upper(col("name")))
df.withColumn("name_lower", lower(col("name")))



7. trim() â†’ âœ‚ï¸ Remove whitespace
 
from pyspark.sql.functions import trim

df.withColumn("clean_name", trim(col("name")))



8. array_contains() â†’ ðŸ” Check if array contains a value
 
from pyspark.sql.functions import array_contains, split

df.withColumn("has_100", array_contains(split(col("marks"), ","), "100"))



9. regexp_replace() â†’ ðŸ” Replace using regex
 
from pyspark.sql.functions import regexp_replace



df.withColumn("clean_marks", regexp_replace(col("marks"), ",", "|"))



10. round() / floor() / ceil() â†’ ðŸ”¢ Numeric operations
 
from pyspark.sql.functions import round, floor, ceil

df.withColumn("rounded_price", round(col("price"), 2))



11. when() / otherwise() â†’ ðŸ§  Conditional logic
 
from pyspark.sql.functions import when

df.withColumn("grade", when(col("marks") > 90, "A").otherwise("B"))



12. year() / month() / dayofmonth() â†’ ðŸ“… Date extract
 
from pyspark.sql.functions import year, month

df.withColumn("birth_year", year(col("dob")))
