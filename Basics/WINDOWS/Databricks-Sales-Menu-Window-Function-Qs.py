



Your detailed schema setup is very helpful! It clarifies the exact column names and data types, especially the presence of the `Source_order` column and the date components you've derived (`order_year`, etc.).

Here are **10 intermediate-level PySpark problems** based on your specific `sales_df` and `menu_df` schemas, focusing on Window Functions, Joins, and Aggregations.

***

## ðŸ§© 10 Intermediate PySpark Problems

### DataFrames and Key Columns:

| DataFrame | Key Columns | Derived Columns |
| :--- | :--- | :--- |
| **`sales_df`** | `Product_id`, `Customer_id`, `Order_date`, `Location`, `Source_order` | `order_year`, `order_month`, `order_quarter` |
| **`menu_df`** | `Product_id`, `Product_name`, `Price` (NOTE: This column is StringType in your schema; we must cast it to numeric for calculations) | |

### Section A: Window Functions (Analytical Tasks)

These problems require defining a `WindowSpec` to calculate metrics relative to groups (partitions).

#### 1. Running Total Sales Per Customer
Calculate the **cumulative sum of total sales** (`Price`) for each `Customer_id`, ordered by `Order_date`. The output should show the lifetime spending of each customer up to every transaction.

#### 2. Rank of Top-Selling Products by Quarter
For **each quarter** (`order_quarter`), rank the top 5 `Product_name`s based on their total revenue generated in that quarter.

#### 3. Monthly Average Sales Benchmark
For every individual transaction, calculate the **average monthly sales** across the entire company for that transaction's specific `order_month` and display how much the individual transaction's `Price` deviates from that monthly average.

#### 4. Top-Spending Customer Per Location
In **each `Location`**, identify the single `Customer_id` that has the highest **total lifetime spending**. (Hint: Requires aggregation, ranking, and then filtering).

***

### Section B: Complex Joins and Aggregation

These problems involve multi-step aggregations and conditional logic.

#### 5. Percentage of Revenue by Source
Calculate the total revenue generated by each `Source_order` (e.g., 'App', 'Website', etc.) and then calculate what **percentage** that revenue contributes to the **overall total company sales**.

#### 6. Average Order Value by Location
Find the average value of a single order (transaction) at **each `Location`**. (You'll need to assume one row per order, or group by a unique `Order_id` if that column were present; given the schema, you may have to assume `Order_date` acts as a unique transaction identifier per customer for this level of granularity, but the standard solution groups by `Location` and takes the average of the price of all items sold).

#### 7. Top Product by Customer Acquisition Year
Identify the single `Product_name` that generated the highest revenue among **customers whose first purchase occurred in 2024** (`order_year` = 2024). (Requires finding the acquisition year first).

***

### Section C: Conditional & Temporal Analysis

These problems leverage the date/time components and conditional logic (`when/otherwise`).

#### 8. Weekend vs. Weekday Sales Analysis
Create a DataFrame summarizing the **total sales difference** between weekday sales (Mon-Fri) and weekend sales (Sat-Sun) for **each `Location`**. (Hint: Use `dayofweek` or similar function).

#### 9. Products Not Sold in a Specific Quarter
Identify all `Product_name`s that were sold at least once in `order_quarter` **1** but **were not sold at all** in `order_quarter` **4**.

#### 10. Customer Reactivation Rate (Simplified)
For all customers who made a purchase in **Q1**, find the count of those customers who made a subsequent purchase in **Q4** of the same year. This identifies reactivated customers. (Requires self-join or advanced filtering).
